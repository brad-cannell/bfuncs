---
title: "Descriptive Analysis in a dplyr Pipeline"
author: "Brad Cannell"
date: "Created: 2017-11-10 <br> Updated: `r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Frequency Tables}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

I initially created this file to help develop the `freq_table` function in `bfuncs`. I've since adapted it to encompass general descriptive data analysis using R in a `dplyr` pipeline. Herein, descriptive analysis refers to basic univariate or bivariate statistics calculated for continuous and categorical variables (e.g., means and percentages). This vignette is not intended to be representative of every possible descriptive analysis that one may want to carry out on a given data set. Rather, it is intended to be representative of the descriptive analyses I most commonly need while conducting epidemiologic research.

```{r}
library(tidyverse)
library(bfuncs)
```

```{r}
data(mtcars)
```

Table of contents:

[Univariate means and 95% confidence intervals]()

[Bivariate means and 95% confidence intervals]()

[Univariate percentages and 95% Wald confidence intervals]()

[Univariate percentages and 95% log transformed confidence intervals]()

[Bivariate percentages and 95% log transformed confidence intervals]()

# Univariate means and 95% confidence intervals

In this example, we will calculate the overall mean and 95% confidence interval for the variable mpg in the mtcars data set. 

By default, only the n, mean, and 95% confidence interval for the mean are returned. Additionally, the values of all the returned statistics are rounded to the hundredths place. These are the numerical summaries of the data that I am most frequently interested in. Additionally, I rarely need the precision of the estimates to be any greater than the hundredths place.

The confidence intervals are calculated as:

$$ {\bar{x} \pm t_{(1-\alpha / 2, n-1)}} \frac{s}{\sqrt{n}} $$

This matches the method used by SAS: http://support.sas.com/documentation/cdl/en/proc/65145/HTML/default/viewer.htm#p0klmrp4k89pz0n1p72t0clpavyx.htm

```{r}
mtcars %>% 
  mean_table(mpg)
```

By adjusting the `t_prob` parameter, it is possible to change the width of the confidence intervals. The example below returns a 99% confidence interval.

The value for t-prob is calculated as 1 - alpha / 2.

```{r}
alpha <- 0.01
t <- 1 - alpha / 2

mtcars %>% 
  mean_table(mpg, t_prob = t)
```

With the `output = "all"` option, mean_table also returns the number of missing values, the critical value from student's t distribution with degrees of freedom n - 1, and the standard error of the mean.

We can also control the precision of the statistics using the `digits` parameter.

```{r}
mtcars %>% 
  mean_table(mpg, output = "all", digits = 5)
```

This output matches the output 

![](/Users/bradcannell/Dropbox/R/Packages/bfuncs/vignettes/mean_mpg.png)

As you can see in the SAS output below...
Type of confidence intervals
Give the type of confidence interval (Wald)



2. Let's just take for granted that we are going to get a grouped dataframe. That's easy enough to implement in R.

Useful websites: 

https://support.sas.com/documentation/cdl/en/statug/63347/HTML/default/viewer.htm#statug_surveyfreq_a0000000221.htm

https://support.sas.com/documentation/cdl/en/statug/63347/HTML/default/viewer.htm#statug_surveyfreq_a0000000217.htm





# One-way frequency tables

With Wald (SAS) CI's

```{r}
mtcars %>% 
  group_by(am) %>% 
  summarise(n = n()) %>% 
  mutate(
    cumsum = sum(n),
    prop = n / cumsum,
    se = sqrt(prop * (1 - prop) / (cumsum - 1)),
    tcrit = stats::qt(0.975, df = cumsum - 1),
    lower = prop - tcrit * se,
    upper = prop + tcrit * se
  )
```

Double-checked with Stata and SAS. The se should be n-1 and the df should be n-1. These confindence limits match SAS exactly.

Using logit transformed confidence intervals like Stata. https://www.stata.com/manuals13/rproportion.pdf

```{r}
mtcars %>% 
  group_by(am) %>% 
  summarise(n = n()) %>% 
    mutate(
      cumsum    = sum(n),
      prop      = n / cumsum,
      log_prop  = log(prop) - log(1 - prop),
      se        = sqrt(prop * (1 - prop) / (cumsum - 1)),
      log_se    = se / (prop * (1 - prop)),
      t_crit    = stats::qt(0.975, df = cumsum - 1),
      log_lower = log_prop - t_crit * log_se,
      log_upper = log_prop + t_crit * log_se,
      lower     = exp(log_lower) / (1 + exp(log_lower)),
      upper     = exp(log_upper) / (1 + exp(log_upper))
    ) %>% 
    select(1, n, prop, se, lower, upper)
```

This matches Stata exactly.





# Two-way frequency tables

```{r}
mtcars %>% 
  group_by(am, cyl) %>% 
  summarise(n = n()) %>% 
  mutate(
    cumsum = sum(n),
    prop = n / cumsum,
    percent = prop * 100,
    se = sqrt(prop * (1 - prop) / (cumsum)),
    sep = se * 100,
    tcrit = stats::qt(0.975, df = cumsum),
    lower = prop - tcrit * se,
    upper = prop + tcrit * se
  ) %>% select(1:4, percent, sep, lower, upper)
```

These don't match SAS. The Standard errors are off. The SAS documentation says they use the Taylor series variance estimation method.

https://support.sas.com/documentation/cdl/en/statug/63347/HTML/default/viewer.htm#statug_surveyfreq_a0000000217.htm

I don't know how to calculate that. Seeing if I can reproduce Stata results.

```{r}
mtcars %>% 
  group_by(am, cyl) %>% 
  summarise(n = n()) %>% 
  mutate(group_n = sum(n)) %>% 
  ungroup() %>%
  mutate(
    total_n   = sum(n),
    prop      = n / group_n,
    log_prop  = log(prop) - log(1 - prop),
    se        = sqrt(prop * (1 - prop) / (group_n - 1)), # group n - 1
    log_se    = se / (prop * (1 - prop)),
    t_crit    = stats::qt(0.975, df = total_n - 1), # overall n - 1
    log_lower = log_prop - t_crit * log_se,
    log_upper = log_prop + t_crit * log_se,
    lower     = exp(log_lower) / (1 + exp(log_lower)),
    upper     = exp(log_upper) / (1 + exp(log_upper))
  ) %>% 
  select(1:2, n, group_n, total_n, prop, se, lower, upper)  
  # map_at(.at = c(5:8), ~ . * 100) %>% 
  # as_tibble()
```

Exact same results as Stata.

Write function. Given a grouped tibble, automatically return the stats above.

```{r}
freq_table <- function(x, t_prob = 0.975, ci_type = "log", ...) {
  
  # ===========================================================================
  # Check for grouped tibble
  # ===========================================================================
  if (!("grouped_df" %in% class(x))) {
    stop(paste("The x argument to freq_table must be a grouped tibble. 
               The class of the current x argument is", class(x)))
  } # No else
  
  # ===========================================================================
  # Check for number of group vars: 
  # If 1 var then use Wald and overall prop
  # If 2 vars then use logit transformation for CI's and give row prop in
  # addition to overall.
  # ===========================================================================
  out <- x %>% 
    summarise(n = n())
  
  # ===========================================================================
  # One-way tables
  # ===========================================================================
  if (ncol(out) == 2) { # else is in two-way tables.
    
    # Update out to include elements need for Wald and Log transformed CI's
    # One-way tables
    out <- out %>% 
      mutate(
        n_total = sum(n),
        prop    = n / n_total,
        se      = sqrt(prop * (1 - prop) / (n_total - 1)),
        t_crit  = stats::qt(t_prob, df = n_total - 1)
      )
        
        # Calculate Wald CI's
        # and put prop, se, and CI's on percent scale
        # One-way tables
        if (ci_type == "wald") {
          
          out <- out %>% 
            mutate(
              lcl_wald = prop - t_crit * se,
              ucl_wald = prop + t_crit * se,
              percent  = prop * 100,
              se       = se * 100, 
              lcl_wald = lcl_wald * 100,
              ucl_wald = ucl_wald * 100
            ) %>% 
            
            # Control output
            select(1, n, n_total, percent, se, t_crit, lcl_wald, ucl_wald)
        
        # Calculate log transformed CI's
        # and put prop, se, and CI's on percent scale
        # One-way tables
        } else if (ci_type == "log") {
          
          out <- out %>%
            mutate(
              prop_log = log(prop) - log(1 - prop),
              se_log   = se / (prop * (1 - prop)),
              lcl_log  = prop_log - t_crit * se_log,
              ucl_log  = prop_log + t_crit * se_log,
              lcl_log  = exp(lcl_log) / (1 + exp(lcl_log)),
              ucl_log  = exp(ucl_log) / (1 + exp(ucl_log)),
              percent  = prop * 100,
              se       = se * 100, 
              lcl_log  = lcl_log * 100,
              ucl_log  = ucl_log * 100 
            ) %>% 
          
            # Control output
            select(1, n, n_total, percent, se, t_crit, lcl_log, ucl_log)
        }


  # =========================================================================== 
  # Two-way tables
  # Only logged transformed CI's
  # Need percent and row percent
  # ===========================================================================
  } else if (ncol(out) == 3) { # if is one-way tables
    
    out <- out %>%
      # Calculate within row n
      mutate(n_group = sum(n)) %>%
      # Ungroup to get total_n
      ungroup() %>%
      mutate(
        
        # Estimate overall percent se and CI's
        n_total        = sum(n),
        prop_total     = n / n_total,
        se_total       = sqrt(prop_total * (1 - prop_total) / (n_total - 1)),
        t_crit_total   = stats::qt(t_prob, df = n_total - 1),
        prop_log_total = log(prop_total) - log(1 - prop_total),
        se_log_total   = se_total / (prop_total * (1 - prop_total)),
        lcl_total_log  = prop_log_total - t_crit_total * se_log_total,
        ucl_total_log  = prop_log_total + t_crit_total * se_log_total,
        lcl_total_log  = exp(lcl_total_log) / (1 + exp(lcl_total_log)),
        ucl_total_log  = exp(ucl_total_log) / (1 + exp(ucl_total_log)),
        percent_total  = prop_total * 100,
        se_total       = se_total * 100, 
        lcl_total_log  = lcl_total_log * 100,
        ucl_total_log  = ucl_total_log * 100,
        
        
        # Estimate row percent se and CI's
        prop_row     = n / n_group,
        se_row       = sqrt(prop_row * (1 - prop_row) / (n_group - 1)), # group n - 1
        t_crit_row   = stats::qt(t_prob, df = n_total - 1), # overall n - 1
        prop_log_row = log(prop_row) - log(1 - prop_row),
        se_log_row   = se_row / (prop_row * (1 - prop_row)),
        lcl_row_log  = prop_log_row - t_crit_row * se_log_row,
        ucl_row_log  = prop_log_row + t_crit_row * se_log_row,
        lcl_row_log  = exp(lcl_row_log) / (1 + exp(lcl_row_log)),
        ucl_row_log  = exp(ucl_row_log) / (1 + exp(ucl_row_log)),
        percent_row  = prop_row * 100,
        se_row       = se_row * 100, 
        lcl_row_log  = lcl_row_log * 100,
        ucl_row_log  = ucl_row_log * 100
      ) %>% 
          
      # Control output
      select(1:2, n, n_group, n_total, percent_total, se_total, lcl_total_log, 
             ucl_total_log, percent_row, se_row, lcl_row_log, ucl_row_log)
    
  } else { # Grouped by more than two variables, or not grouped.
    stop(
      paste(
        "Expecting x to be a grouped data frame with 2 or 3 columns. Instead
        x had", ncol(out) 
      )
    )
  }
  
  # Return tibble of results
  out
}
```

Test one_table

```{r}
mtcars %>% 
  group_by(am) %>% 
  freq_table()
```

Test two-way tables

```{r}
mtcars %>% 
  group_by(am, cyl) %>% 
  freq_table()
```

Data checks:    
* One-way Wald matches SAS.    
* One-way log transformed matches Stata.    
* Two-way log transformed matches Stata.    
* Need to see if I can get someone in biostats to help me code the Taylor series.
